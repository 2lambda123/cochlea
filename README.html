<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="Docutils 0.12: http://docutils.sourceforge.net/" />
<title>cochlea</title>
<style type="text/css">

/*
:Author: David Goodger (goodger@python.org)
:Id: $Id: html4css1.css 7614 2013-02-21 15:55:51Z milde $
:Copyright: This stylesheet has been placed in the public domain.

Default cascading style sheet for the HTML output of Docutils.

See http://docutils.sf.net/docs/howto/html-stylesheets.html for how to
customize this style sheet.
*/

/* used to remove borders from tables and images */
.borderless, table.borderless td, table.borderless th {
  border: 0 }

table.borderless td, table.borderless th {
  /* Override padding for "table.docutils td" with "! important".
     The right padding separates the table cells. */
  padding: 0 0.5em 0 0 ! important }

.first {
  /* Override more specific margin styles with "! important". */
  margin-top: 0 ! important }

.last, .with-subtitle {
  margin-bottom: 0 ! important }

.hidden {
  display: none }

a.toc-backref {
  text-decoration: none ;
  color: black }

blockquote.epigraph {
  margin: 2em 5em ; }

dl.docutils dd {
  margin-bottom: 0.5em }

object[type="image/svg+xml"], object[type="application/x-shockwave-flash"] {
  overflow: hidden;
}

/* Uncomment (and remove this text!) to get bold-faced definition list terms
dl.docutils dt {
  font-weight: bold }
*/

div.abstract {
  margin: 2em 5em }

div.abstract p.topic-title {
  font-weight: bold ;
  text-align: center }

div.admonition, div.attention, div.caution, div.danger, div.error,
div.hint, div.important, div.note, div.tip, div.warning {
  margin: 2em ;
  border: medium outset ;
  padding: 1em }

div.admonition p.admonition-title, div.hint p.admonition-title,
div.important p.admonition-title, div.note p.admonition-title,
div.tip p.admonition-title {
  font-weight: bold ;
  font-family: sans-serif }

div.attention p.admonition-title, div.caution p.admonition-title,
div.danger p.admonition-title, div.error p.admonition-title,
div.warning p.admonition-title, .code .error {
  color: red ;
  font-weight: bold ;
  font-family: sans-serif }

/* Uncomment (and remove this text!) to get reduced vertical space in
   compound paragraphs.
div.compound .compound-first, div.compound .compound-middle {
  margin-bottom: 0.5em }

div.compound .compound-last, div.compound .compound-middle {
  margin-top: 0.5em }
*/

div.dedication {
  margin: 2em 5em ;
  text-align: center ;
  font-style: italic }

div.dedication p.topic-title {
  font-weight: bold ;
  font-style: normal }

div.figure {
  margin-left: 2em ;
  margin-right: 2em }

div.footer, div.header {
  clear: both;
  font-size: smaller }

div.line-block {
  display: block ;
  margin-top: 1em ;
  margin-bottom: 1em }

div.line-block div.line-block {
  margin-top: 0 ;
  margin-bottom: 0 ;
  margin-left: 1.5em }

div.sidebar {
  margin: 0 0 0.5em 1em ;
  border: medium outset ;
  padding: 1em ;
  background-color: #ffffee ;
  width: 40% ;
  float: right ;
  clear: right }

div.sidebar p.rubric {
  font-family: sans-serif ;
  font-size: medium }

div.system-messages {
  margin: 5em }

div.system-messages h1 {
  color: red }

div.system-message {
  border: medium outset ;
  padding: 1em }

div.system-message p.system-message-title {
  color: red ;
  font-weight: bold }

div.topic {
  margin: 2em }

h1.section-subtitle, h2.section-subtitle, h3.section-subtitle,
h4.section-subtitle, h5.section-subtitle, h6.section-subtitle {
  margin-top: 0.4em }

h1.title {
  text-align: center }

h2.subtitle {
  text-align: center }

hr.docutils {
  width: 75% }

img.align-left, .figure.align-left, object.align-left {
  clear: left ;
  float: left ;
  margin-right: 1em }

img.align-right, .figure.align-right, object.align-right {
  clear: right ;
  float: right ;
  margin-left: 1em }

img.align-center, .figure.align-center, object.align-center {
  display: block;
  margin-left: auto;
  margin-right: auto;
}

.align-left {
  text-align: left }

.align-center {
  clear: both ;
  text-align: center }

.align-right {
  text-align: right }

/* reset inner alignment in figures */
div.align-right {
  text-align: inherit }

/* div.align-center * { */
/*   text-align: left } */

ol.simple, ul.simple {
  margin-bottom: 1em }

ol.arabic {
  list-style: decimal }

ol.loweralpha {
  list-style: lower-alpha }

ol.upperalpha {
  list-style: upper-alpha }

ol.lowerroman {
  list-style: lower-roman }

ol.upperroman {
  list-style: upper-roman }

p.attribution {
  text-align: right ;
  margin-left: 50% }

p.caption {
  font-style: italic }

p.credits {
  font-style: italic ;
  font-size: smaller }

p.label {
  white-space: nowrap }

p.rubric {
  font-weight: bold ;
  font-size: larger ;
  color: maroon ;
  text-align: center }

p.sidebar-title {
  font-family: sans-serif ;
  font-weight: bold ;
  font-size: larger }

p.sidebar-subtitle {
  font-family: sans-serif ;
  font-weight: bold }

p.topic-title {
  font-weight: bold }

pre.address {
  margin-bottom: 0 ;
  margin-top: 0 ;
  font: inherit }

pre.literal-block, pre.doctest-block, pre.math, pre.code {
  margin-left: 2em ;
  margin-right: 2em }

pre.code .ln { color: grey; } /* line numbers */
pre.code, code { background-color: #eeeeee }
pre.code .comment, code .comment { color: #5C6576 }
pre.code .keyword, code .keyword { color: #3B0D06; font-weight: bold }
pre.code .literal.string, code .literal.string { color: #0C5404 }
pre.code .name.builtin, code .name.builtin { color: #352B84 }
pre.code .deleted, code .deleted { background-color: #DEB0A1}
pre.code .inserted, code .inserted { background-color: #A3D289}

span.classifier {
  font-family: sans-serif ;
  font-style: oblique }

span.classifier-delimiter {
  font-family: sans-serif ;
  font-weight: bold }

span.interpreted {
  font-family: sans-serif }

span.option {
  white-space: nowrap }

span.pre {
  white-space: pre }

span.problematic {
  color: red }

span.section-subtitle {
  /* font-size relative to parent (h1..h6 element) */
  font-size: 80% }

table.citation {
  border-left: solid 1px gray;
  margin-left: 1px }

table.docinfo {
  margin: 2em 4em }

table.docutils {
  margin-top: 0.5em ;
  margin-bottom: 0.5em }

table.footnote {
  border-left: solid 1px black;
  margin-left: 1px }

table.docutils td, table.docutils th,
table.docinfo td, table.docinfo th {
  padding-left: 0.5em ;
  padding-right: 0.5em ;
  vertical-align: top }

table.docutils th.field-name, table.docinfo th.docinfo-name {
  font-weight: bold ;
  text-align: left ;
  white-space: nowrap ;
  padding-left: 0 }

/* "booktabs" style (no vertical lines) */
table.docutils.booktabs {
  border: 0px;
  border-top: 2px solid;
  border-bottom: 2px solid;
  border-collapse: collapse;
}
table.docutils.booktabs * {
  border: 0px;
}
table.docutils.booktabs th {
  border-bottom: thin solid;
  text-align: left;
}

h1 tt.docutils, h2 tt.docutils, h3 tt.docutils,
h4 tt.docutils, h5 tt.docutils, h6 tt.docutils {
  font-size: 100% }

ul.auto-toc {
  list-style-type: none }

</style>
</head>
<body>
<div class="document" id="cochlea">
<h1 class="title">cochlea</h1>

<p><em>cochlea</em> is a collection of inner ear models.  All models are easily
accessible as Python functions.  They take sound signal as input and
return <a class="reference external" href="https://en.wikipedia.org/wiki/Spike_train">spike trains</a> of the auditory nerve fibers:</p>
<pre class="literal-block">
                         +-----------+     __|______|______|____
 .-.     .-.     .-.     |           |--&gt;  _|________|______|___
/   \   /   \   /   \ --&gt;|  Cochlea  |--&gt;  ___|______|____|_____
     '-'     '-'         |           |--&gt;  __|______|______|____
                         +-----------+
          Sound                               Spike Trains
                                            (Auditory Nerve)
</pre>
<p>The package contains state-of-the-art biophysical models, which give
realistic approximation of the auditory nerve activity.</p>
<p>The models are implemented using the original code from their authors
whenever possible.  Therefore, they return the same results as the
original models.  We made an effort to verify it with unit testing
(see tests directory for details).</p>
<p>The implementation is also fast.  It is easy to generate responses of
hundreds or even thousands of auditory nerve fibers (ANFs).  It is
possible, for example, to generate responses of the whole human
auditory nerve (around 30,000 ANFs).  We usually tested the models
with sounds up to 1 second in duration.</p>
<p>I developed <em>cochlea</em> during my PhD in the group of Werner Hemmert
(<a class="reference external" href="http://www.imetum.tum.de/research/bai/home/?L=1">Bio-Inspired Information Processing</a>) at the TUM.  It went through
several versions and rewrites.  Now, it is quite stable and we decided
to release it for the community.</p>
<p><em>cochlea</em> could be used by:</p>
<ul class="simple">
<li>researchers doing computational neuroscience (combines well with
<a class="reference external" href="http://www.neuron.yale.edu/neuron/">NEURON</a> and <a class="reference external" href="http://briansimulator.org/">Brian</a>),</li>
<li>experimenters that require realistic input spike trains,</li>
<li>researchers improving inner ear models,</li>
<li>everyone interested in understanding how hearing works.</li>
</ul>
<div class="section" id="implemented-models">
<h1>Implemented Models</h1>
<ul class="simple">
<li>Holmberg, M. (2007). Speech Encoding in the Human Auditory
Periphery: Modeling and Quantitative Assessment by Means of
Automatic Speech Recognition. PhD thesis, Technical University
Darmstadt.</li>
<li>Zilany, M. S., Bruce, I. C., Nelson, P. C., &amp;
Carney, L. H. (2009). A phenomenological model of the synapse
between the inner hair cell and auditory nerve: long-term adaptation
with power-law dynamics. The Journal of the Acoustical Society of
America, 126(5), 2390-2412.</li>
<li>Zilany, M. S., Bruce, I. C., &amp; Carney, L. H. (2014). Updated
parameters and expanded simulation options for a model of the
auditory periphery. The Journal of the Acoustical Society of
America, 135(1), 283-286.</li>
<li><a class="reference external" href="http://www.essexpsychology.macmate.me/HearingLab/modelling.html">MATLAB Auditory Periphery</a> by Meddis et al. (external model, not
implemented in the package, but easily accessible through
<a class="reference external" href="https://github.com/mrkrd/matlab_wrapper">matlab_wrapper</a>).</li>
</ul>
</div>
<div class="section" id="usage">
<h1>Usage</h1>
<p>Check our online <a class="reference external" href="http://nbviewer.ipython.org/github/mrkrd/cochlea/blob/master/examples/cochlea_demo.ipynb">DEMO</a> and <a class="reference external" href="https://github.com/mrkrd/cochlea/tree/master/examples">examples</a> (probably the easiest is to start
with <a class="reference external" href="https://github.com/mrkrd/cochlea/blob/master/examples/run_zilany2014.py">run_zilany2014.py</a>).</p>
<p>Initialize the modules:</p>
<pre class="literal-block">
import cochlea
import thorns as th
import thorns.waves as wv
</pre>
<p>Generate sound:</p>
<pre class="literal-block">
fs = 100e3
sound = wv.ramped_tone(
    fs=fs,
    freq=1000,
    duration=0.1,
    dbspl=50
)
</pre>
<p>Run the model (responses of 200 cat HSR fibers):</p>
<pre class="literal-block">
anf_trains = cochlea.run_zilany2014(
    sound,
    fs,
    anf_num=(200,0,0),
    cf=1000,
    seed=0,
    species='cat'
)
</pre>
<p>Plot the results:</p>
<pre class="literal-block">
th.plot_raster(anf_trains)
th.show()
</pre>
<p>You can browse through the API documentation at:
<a class="reference external" href="https://pythonhosted.org/cochlea/">https://pythonhosted.org/cochlea/</a></p>
</div>
<div class="section" id="installation">
<h1>Installation</h1>
<pre class="literal-block">
pip install cochlea
</pre>
<p>Check <a class="reference external" href="INSTALL.rst">INSTALL.rst</a> for details.</p>
</div>
<div class="section" id="spike-train-format">
<h1>Spike Train Format</h1>
<p>Spike train data format is based on a standard <a class="reference external" href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html">DataFrame</a> format from
the excellent <a class="reference external" href="http://pandas.pydata.org/">pandas</a> library.  Spike trains and their meta data are
stored in DataFrame, where each row corresponds to a single neuron:</p>
<table border="1" class="docutils">
<colgroup>
<col width="7%" />
<col width="11%" />
<col width="6%" />
<col width="6%" />
<col width="70%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">index</th>
<th class="head">duration</th>
<th class="head">type</th>
<th class="head">cf</th>
<th class="head">spikes</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>0</td>
<td>0.15</td>
<td>hsr</td>
<td>8000</td>
<td>[0.00243, 0.00414, 0.00715, 0.01089, 0.01358, ...</td>
</tr>
<tr><td>1</td>
<td>0.15</td>
<td>hsr</td>
<td>8000</td>
<td>[0.00325, 0.01234, 0.0203, 0.02295, 0.0268, 0....</td>
</tr>
<tr><td>2</td>
<td>0.15</td>
<td>hsr</td>
<td>8000</td>
<td>[0.00277, 0.00594, 0.01104, 0.01387, 0.0234, 0...</td>
</tr>
<tr><td>3</td>
<td>0.15</td>
<td>hsr</td>
<td>8000</td>
<td>[0.00311, 0.00563, 0.00971, 0.0133, 0.0177, 0....</td>
</tr>
<tr><td>4</td>
<td>0.15</td>
<td>hsr</td>
<td>8000</td>
<td>[0.00283, 0.00469, 0.00929, 0.01099, 0.01779, ...</td>
</tr>
<tr><td>5</td>
<td>0.15</td>
<td>hsr</td>
<td>8000</td>
<td>[0.00352, 0.00781, 0.01138, 0.02166, 0.02575, ...</td>
</tr>
<tr><td>6</td>
<td>0.15</td>
<td>hsr</td>
<td>8000</td>
<td>[0.00395, 0.00651, 0.00984, 0.0157, 0.02209, 0...</td>
</tr>
<tr><td>7</td>
<td>0.15</td>
<td>hsr</td>
<td>8000</td>
<td>[0.00385, 0.009, 0.01537, 0.02114, 0.02377, 0....</td>
</tr>
</tbody>
</table>
<p>The column 'spikes' is the most important and stores an array with
spike times (time stamps) in seconds for every action potential.  The
column 'duration' is the duration of the sound.  The column 'cf' is
the characteristic frequency (CF) of the fiber.  The column 'type'
tells us what auditory nerve fiber generated the spike train.  'hsr'
is for high-spontaneous rate fiber, 'msr' and 'lsr' for medium- and
low-spontaneous rate fibers.</p>
<p>Advantages of the format:</p>
<ul>
<li><p class="first">easy addition of new meta data,</p>
</li>
<li><p class="first">efficient grouping and filtering of trains using _DataFrame
functionality,</p>
</li>
<li><p class="first">export to MATLAB struct array through mat files:</p>
<pre class="literal-block">
scipy.io.savemat(
    &quot;spikes.mat&quot;,
    {'spike_trains': spike_trains.to_records()}
)
</pre>
</li>
</ul>
<p>Please, check <a class="reference external" href="https://github.com/mrkrd/thorns">thorns</a> for more information and functions to manipulate
spike trains.</p>
</div>
<div class="section" id="contribute">
<h1>Contribute</h1>
<ul class="simple">
<li>Issue Tracker: <a class="reference external" href="https://github.com/mrkrd/cochlea/issues">https://github.com/mrkrd/cochlea/issues</a></li>
<li>Source Code: <a class="reference external" href="https://github.com/mrkrd/cochlea">https://github.com/mrkrd/cochlea</a></li>
</ul>
</div>
<div class="section" id="support">
<h1>Support</h1>
<p>If you are having issues, please let us know.  We have a mailing list
located at: <a class="reference external" href="mailto:cochlea-and-thorns&#64;googlegroups.com">cochlea-and-thorns&#64;googlegroups.com</a></p>
</div>
<div class="section" id="other-implementations">
<h1>Other Implementations</h1>
<ul class="simple">
<li><a class="reference external" href="http://www.urmc.rochester.edu/labs/Carney-Lab/publications/auditory-models.cfm">Carney Lab</a></li>
<li><a class="reference external" href="http://www.essexpsychology.macmate.me/HearingLab/modelling.html">Matlab Auditory Periphery</a></li>
<li><a class="reference external" href="http://dsam.org.uk/">DSAM</a></li>
<li><a class="reference external" href="http://www.briansimulator.org/docs/hears.html">Brian Hears</a></li>
<li><a class="reference external" href="http://amtoolbox.sourceforge.net/">The Auditory Modeling Toolbox</a></li>
</ul>
</div>
<div class="section" id="citing">
<h1>Citing</h1>
<p>If you are using this software in your research, please make a
reference: Rudnicki, M. and Hemmert, W. (2014) <em>Cochlea: inner ear
models in Python</em>, <a class="reference external" href="https://github.com/mrkrd/cochlea">https://github.com/mrkrd/cochlea</a>.</p>
<p>When you use any of the models, always cite the original publications
describing the model.</p>
</div>
<div class="section" id="acknowledgments">
<h1>Acknowledgments</h1>
<p>We would like to thank Muhammad S.A. Zilany, Ian C. Bruce and
Laurel H. Carney for developing inner ear models and allowing us to
use their code in <em>cochlea</em>.</p>
<p>Thanks goes to Marcus Holmberg, who developed the traveling wave based
model.  His work was supported by the General Federal Ministry of
Education and Research within the Munich Bernstein Center for
Computational Neuroscience (reference No. 01GQ0441, 01GQ0443 and
01GQ1004B).</p>
<p>We are grateful to Ray Meddis for support with the Matlab Auditory
Periphery model.</p>
<p>And last, but not least, I would like to thank Werner Hemmert for
supervising my PhD.</p>
<p>This work was supported by the General Federal Ministry of Education
and Research within the Munich Bernstein Center for Computational
Neuroscience (reference No. 01GQ0441 and 01GQ1004B) and the German
Research Foundation Foundation's Priority Program PP 1608 <em>Ultrafast
and temporally precise information processing: Normal and
dysfunctional hearing</em>.</p>
</div>
<div class="section" id="license">
<h1>License</h1>
<p>The project is licensed under the GNU General Public License v3 or
later (GPLv3+).</p>
</div>
</div>
</body>
</html>
